{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fea3a2b3",
   "metadata": {},
   "source": [
    "# H&M Recommendation System â€” 04 SVD Graph Embedding\n",
    "\n",
    "In this notebook, based on `03_ranking.ipynb`to build item-item co-occurrence graph from train_hist and compute TruncatedSVD embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32ce0e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guest/r12944014/miniconda3/envs/hm_rec/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from collections import defaultdict\n",
    "from scipy import sparse\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "DATA_DIR = \"../data\"\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, \"train\")\n",
    "RECALL_DIR = os.path.join(DATA_DIR, \"recall\")\n",
    "HM_DATA_DIR = \"../hm_data\"\n",
    "\n",
    "OUT_DIR = \"../experiments\"\n",
    "MODEL_DIR = \"../models\"\n",
    "CFG_DIR = \"../experiments/configs\"\n",
    "RESULT_CSV = \"../experiments/results.csv\"\n",
    "\n",
    "os.makedirs(TRAIN_DIR, exist_ok=True)\n",
    "os.makedirs(HM_DATA_DIR, exist_ok=True)\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(CFG_DIR, exist_ok=True)\n",
    "\n",
    "VALID_START = pd.to_datetime(\"2020-09-16\")\n",
    "VALID_END = pd.to_datetime(\"2020-09-23\")  # exclusive\n",
    "\n",
    "# Internal train label window (last 7 days before VALID_START)\n",
    "TRAIN_LABEL_START = VALID_START - pd.Timedelta(days=7)\n",
    "\n",
    "# Recall sizes\n",
    "N_HISTORY = 30\n",
    "N_POP = 20\n",
    "N_CATEGORY = 20\n",
    "N_COPURCHASE = 30\n",
    "MAX_CANDIDATES = 100   # final merge cap per user\n",
    "\n",
    "# Category mapping source column\n",
    "CATEGORY_COL = \"product_type_no\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pickle(obj, path, overwrite=False):\n",
    "    if (not overwrite) and os.path.exists(path):\n",
    "        print(f\"[skip] exists: {path}\")\n",
    "        return\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(obj, f)\n",
    "    print(f\"[saved] {path}\")\n",
    "\n",
    "def load_pickle(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def save_json(obj, path, overwrite=False):\n",
    "    if (not overwrite) and os.path.exists(path):\n",
    "        print(f\"[skip] exists: {path}\")\n",
    "        return\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"[saved] {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train window: 2018-09-20 00:00:00 -> 2020-09-15 00:00:00 (31548013, 5)\n",
      "Valid window: 2020-09-16 00:00:00 -> 2020-09-22 00:00:00 (240311, 5)\n"
     ]
    }
   ],
   "source": [
    "train_df = load_pickle(os.path.join(TRAIN_DIR, \"train_df.pkl\"))\n",
    "valid_df = load_pickle(os.path.join(TRAIN_DIR, \"valid_df.pkl\"))\n",
    "\n",
    "# Make sure types are correct\n",
    "train_df[\"t_dat\"] = pd.to_datetime(train_df[\"t_dat\"])\n",
    "valid_df[\"t_dat\"] = pd.to_datetime(valid_df[\"t_dat\"])\n",
    "\n",
    "print(\"Train window:\", train_df[\"t_dat\"].min(), \"->\", train_df[\"t_dat\"].max(), train_df.shape)\n",
    "print(\"Valid window:\", valid_df[\"t_dat\"].min(), \"->\", valid_df[\"t_dat\"].max(), valid_df.shape)\n",
    "\n",
    "# ground truth dicts: customer -> set(article_id)\n",
    "train_hist = train_df[train_df[\"t_dat\"] < TRAIN_LABEL_START].copy()\n",
    "train_label_period = train_df[(train_df[\"t_dat\"] >= TRAIN_LABEL_START) & (train_df[\"t_dat\"] < VALID_START)].copy()\n",
    "train_gt = train_label_period.groupby(\"customer_id\")[\"article_id\"].apply(lambda s: set(s.astype(int))).to_dict()\n",
    "valid_period = valid_df[(valid_df[\"t_dat\"] >= VALID_START) & (valid_df[\"t_dat\"] < VALID_END)].copy()\n",
    "valid_gt = valid_period.groupby(\"customer_id\")[\"article_id\"].apply(lambda s: set(s.astype(int))).to_dict()\n",
    "\n",
    "cust_to_candidates = load_pickle(os.path.join(RECALL_DIR, \"recall_final_merged.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8acb99cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_path = os.path.join(HM_DATA_DIR, \"articles.csv\")\n",
    "articles = pd.read_csv(articles_path)\n",
    "if \"product_group_id\" not in articles.columns:\n",
    "    articles[\"product_group_id\"] = pd.factorize(articles[\"product_group_name\"])[0].astype(np.int32)\n",
    "articles_use = articles[[\n",
    "    \"article_id\",\n",
    "    \"product_type_no\",\n",
    "    \"product_group_id\",\n",
    "    \"colour_group_code\",\n",
    "    \"department_no\",\n",
    "    \"index_group_no\",\n",
    "    \"garment_group_no\",\n",
    "]].copy()\n",
    "articles_use[\"article_id\"] = articles_use[\"article_id\"].astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91f55a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_df_train = load_pickle(os.path.join(TRAIN_DIR, \"rank_df_train.pkl\"))\n",
    "rank_df_val = load_pickle(os.path.join(TRAIN_DIR, \"rank_df_valid.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f5d9980",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_df_train_f = load_pickle(os.path.join(TRAIN_DIR, \"rank_df_train_features.pkl\"))\n",
    "rank_df_val_f = load_pickle(os.path.join(TRAIN_DIR, \"rank_df_valid_features.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13a50aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_item_graph_svd_embeddings(\n",
    "    train_hist: pd.DataFrame,\n",
    "    k: int = 16,\n",
    "    top_items: int = 20000,\n",
    "    user_max_items: int = 20,\n",
    "    min_item_cnt: int = 5,\n",
    "    seed: int = 42\n",
    "):\n",
    "    \"\"\"\n",
    "    Build item-item co-occurrence graph from train_hist and compute TruncatedSVD embeddings.\n",
    "\n",
    "    Speed controls:\n",
    "      - top_items: restrict to top-N popular items\n",
    "      - user_max_items: for each user, only take last N unique items\n",
    "      - min_item_cnt: remove ultra-rare items before top-N\n",
    "    \"\"\"\n",
    "    df = train_hist.copy()\n",
    "    df[\"t_dat\"] = pd.to_datetime(df[\"t_dat\"])\n",
    "    df[\"article_id\"] = df[\"article_id\"].astype(int)\n",
    "\n",
    "    # (1) choose item vocabulary from train_hist\n",
    "    item_cnt = df[\"article_id\"].value_counts()\n",
    "    item_cnt = item_cnt[item_cnt >= min_item_cnt]\n",
    "    vocab_items = item_cnt.head(top_items).index.astype(int).tolist()\n",
    "    vocab_set = set(vocab_items)\n",
    "\n",
    "    item2idx = {aid: i for i, aid in enumerate(vocab_items)}\n",
    "    idx2item = np.array(vocab_items, dtype=np.int64)\n",
    "\n",
    "    print(f\"[graph] vocab size = {len(vocab_items)} (top_items={top_items}, min_item_cnt={min_item_cnt})\")\n",
    "\n",
    "    # (2) build per-user recent unique items (filtered to vocab)\n",
    "    # sort by time then take last unique\n",
    "    df = df.sort_values([\"customer_id\", \"t_dat\"])\n",
    "    # keep only vocab items early to reduce memory\n",
    "    df = df[df[\"article_id\"].isin(vocab_set)]\n",
    "\n",
    "    # get per-user list of items in chronological order, then keep last unique items\n",
    "    # We'll do an efficient pass with groupby apply\n",
    "    def last_unique_tail(s, n=user_max_items):\n",
    "        # s is series of article_id in time order\n",
    "        # keep last occurrence unique\n",
    "        x = pd.Series(s.values)\n",
    "        x = x.drop_duplicates(keep=\"last\")\n",
    "        return x.tail(n).tolist()\n",
    "\n",
    "    user_items = df.groupby(\"customer_id\")[\"article_id\"].apply(last_unique_tail)\n",
    "\n",
    "    print(f\"[graph] users with at least 1 vocab item = {len(user_items)}\")\n",
    "    print(f\"[graph] avg items/user used for graph = {np.mean([len(x) for x in user_items]):.2f}\")\n",
    "\n",
    "    # (3) co-occurrence counting into sparse COO\n",
    "    # We'll count pairs (i,j) for each user's item list (unique)\n",
    "    rows = []\n",
    "    cols = []\n",
    "    data = []\n",
    "\n",
    "    # Use dict for counts to avoid huge COO duplicates\n",
    "    pair_cnt = defaultdict(int)\n",
    "\n",
    "    for items in user_items:\n",
    "        if len(items) < 2:\n",
    "            continue\n",
    "        idxs = sorted(item2idx[i] for i in items if i in item2idx)\n",
    "        m = len(idxs)\n",
    "        for a in range(m):\n",
    "            ia = idxs[a]\n",
    "            for b in range(a + 1, m):\n",
    "                ib = idxs[b]\n",
    "                pair_cnt[(ia, ib)] += 1\n",
    "\n",
    "    print(f\"[graph] unique co-occurrence pairs = {len(pair_cnt)}\")\n",
    "\n",
    "    for (ia, ib), w in pair_cnt.items():\n",
    "        # add symmetric entries\n",
    "        rows.extend([ia, ib])\n",
    "        cols.extend([ib, ia])\n",
    "        data.extend([w, w])\n",
    "\n",
    "    n = len(vocab_items)\n",
    "    A = sparse.coo_matrix((data, (rows, cols)), shape=(n, n), dtype=np.float32).tocsr()\n",
    "\n",
    "    # (4) normalize rows (optional but usually helps)\n",
    "    # Use log1p to dampen huge counts\n",
    "    A = A.copy()\n",
    "    A.data = np.log1p(A.data)\n",
    "\n",
    "    # (5) TruncatedSVD\n",
    "    svd = TruncatedSVD(n_components=k, random_state=seed)\n",
    "    emb = svd.fit_transform(A)  # (n_items, k)\n",
    "\n",
    "    # L2 normalize embeddings (good for cosine)\n",
    "    norm = np.linalg.norm(emb, axis=1, keepdims=True) + 1e-12\n",
    "    emb = emb / norm\n",
    "\n",
    "    item_emb_df = pd.DataFrame(emb, columns=[f\"gsvd_{d}\" for d in range(k)])\n",
    "    item_emb_df[\"article_id\"] = idx2item\n",
    "    item_emb_df = item_emb_df[[\"article_id\"] + [f\"gsvd_{d}\" for d in range(k)]]\n",
    "\n",
    "    print(\"[graph] explained variance ratio (sum):\", float(np.sum(svd.explained_variance_ratio_)))\n",
    "\n",
    "    return item_emb_df, item2idx, idx2item, svd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8804163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[graph] vocab size = 20000 (top_items=20000, min_item_cnt=5)\n",
      "[graph] users with at least 1 vocab item = 1275520\n",
      "[graph] avg items/user used for graph = 9.65\n",
      "[graph] unique co-occurrence pairs = 45494204\n",
      "[graph] explained variance ratio (sum): 0.4089445471763611\n",
      "[saved] ../data/train/item_graph_svd_emb16.pkl\n"
     ]
    }
   ],
   "source": [
    "ITEM_EMB_16_DF, item2idx, idx2item, svd_model = build_item_graph_svd_embeddings(\n",
    "    train_hist=train_hist,\n",
    "    k=16,\n",
    "    top_items=20000,      # tweak: 10000 faster, 30000 richer\n",
    "    user_max_items=20,    # tweak: 10 faster, 30 richer\n",
    "    min_item_cnt=5,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "save_pickle(ITEM_EMB_16_DF, os.path.join(TRAIN_DIR, \"item_graph_svd_emb16.pkl\"), overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee5a9ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_user_profile_from_item_emb(\n",
    "    train_hist: pd.DataFrame,\n",
    "    item_emb_df: pd.DataFrame,\n",
    "    k: int = 16,\n",
    "    user_max_items: int = 20\n",
    "):\n",
    "    \"\"\"\n",
    "    For each user, build a k-dim profile embedding by averaging embeddings of recent unique items (from train_hist).\n",
    "    \"\"\"\n",
    "    df = train_hist.copy()\n",
    "    df[\"t_dat\"] = pd.to_datetime(df[\"t_dat\"])\n",
    "    df[\"article_id\"] = df[\"article_id\"].astype(int)\n",
    "\n",
    "    # map item -> embedding\n",
    "    emb_cols = [f\"gsvd_{d}\" for d in range(k)]\n",
    "    item_emb_map = item_emb_df.set_index(\"article_id\")[emb_cols]\n",
    "\n",
    "    # keep only items with embeddings\n",
    "    df = df[df[\"article_id\"].isin(item_emb_map.index)]\n",
    "    df = df.sort_values([\"customer_id\", \"t_dat\"])\n",
    "\n",
    "    def last_unique_tail(s, n=user_max_items):\n",
    "        x = pd.Series(s.values)\n",
    "        x = x.drop_duplicates(keep=\"last\")\n",
    "        return x.tail(n).tolist()\n",
    "\n",
    "    user_items = df.groupby(\"customer_id\")[\"article_id\"].apply(last_unique_tail)\n",
    "\n",
    "    rows = []\n",
    "    for cust, items in user_items.items():\n",
    "        embs = item_emb_map.loc[items].values  # (m,k)\n",
    "        prof = embs.mean(axis=0)\n",
    "        # L2 normalize\n",
    "        prof = prof / (np.linalg.norm(prof) + 1e-12)\n",
    "        row = {\"customer_id\": cust}\n",
    "        for d in range(k):\n",
    "            row[f\"user_gsvd_{d}\"] = float(prof[d])\n",
    "        rows.append(row)\n",
    "\n",
    "    user_prof = pd.DataFrame(rows)\n",
    "    return user_prof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e42a16b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_601628/315557819.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m USER_PROF_16 = build_user_profile_from_item_emb(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtrain_hist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_hist\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mitem_emb_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mITEM_EMB_16_DF\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_601628/2012378967.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(train_hist, item_emb_df, k, user_max_items)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"last\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0muser_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"customer_id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"article_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_unique_tail\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcust\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitems\u001b[0m \u001b[0;32min\u001b[0m \u001b[0muser_items\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/hm_rec/lib/python3.10/site-packages/pandas/core/groupby/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"series\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_apply_docs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"series_examples\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         )\n\u001b[1;32m    228\u001b[0m     )\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/hm_rec/lib/python3.10/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, func, include_groups, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1835\u001b[0m                         ),\n\u001b[1;32m   1836\u001b[0m                         \u001b[0mcategory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1837\u001b[0m                         \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1838\u001b[0m                     )\n\u001b[0;32m-> 1839\u001b[0;31m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1840\u001b[0m                 \u001b[0;31m# gh-20949\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1841\u001b[0m                 \u001b[0;31m# try again, with .apply acting as a filtering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m                 \u001b[0;31m# operation, by excluding the grouping column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/hm_rec/lib/python3.10/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, f, data, not_indexed_same, is_transform, is_agg)\u001b[0m\n\u001b[1;32m   1883\u001b[0m         \u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1884\u001b[0m         \u001b[0mSeries\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1885\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mapplying\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1886\u001b[0m         \"\"\"\n\u001b[0;32m-> 1887\u001b[0;31m         \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmutated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grouper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_groupwise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1888\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnot_indexed_same\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1889\u001b[0m             \u001b[0mnot_indexed_same\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmutated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/hm_rec/lib/python3.10/site-packages/pandas/core/groupby/ops.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, f, data, axis)\u001b[0m\n\u001b[1;32m    924\u001b[0m             \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"name\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m             \u001b[0;31m# group might be modified\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m             \u001b[0mgroup_axes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 928\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    929\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmutated\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_indexed_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_axes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m                 \u001b[0mmutated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m             \u001b[0mresult_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_601628/2012378967.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(s, n)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlast_unique_tail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_max_items\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"last\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/hm_rec/lib/python3.10/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   5992\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5993\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5994\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5995\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5996\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/hm_rec/lib/python3.10/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1188\u001b[0m             \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_deprecated_callable_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaybe_callable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1192\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/hm_rec/lib/python3.10/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1726\u001b[0m                 \u001b[0;34m\"Consider using .loc for automatic alignment.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1727\u001b[0m             )\n\u001b[1;32m   1728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1729\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1730\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_slice_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1732\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1733\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/hm_rec/lib/python3.10/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, slice_obj, axis)\u001b[0m\n\u001b[1;32m   1762\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1765\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_positional_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1766\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/hm_rec/lib/python3.10/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, slobj, axis)\u001b[0m\n\u001b[1;32m   1101\u001b[0m         \u001b[0;31m#  _slice is *always* positional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m         \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor_from_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmgr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1105\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/hm_rec/lib/python3.10/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, other, method, **kwargs)\u001b[0m\n\u001b[1;32m   6282\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6284\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallows_duplicate_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallows_duplicate_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6285\u001b[0m             \u001b[0;31m# For subclasses using _metadata.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6286\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metadata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6287\u001b[0m                 \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6288\u001b[0m                 \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "USER_PROF_16 = build_user_profile_from_item_emb(\n",
    "    train_hist=train_hist,\n",
    "    item_emb_df=ITEM_EMB_16_DF,\n",
    "    k=16,\n",
    "    user_max_items=20\n",
    ")\n",
    "\n",
    "save_pickle(USER_PROF_16, os.path.join(TRAIN_DIR, \"user_graph_profile_emb16.pkl\"), overwrite=True)\n",
    "USER_PROF_16.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5530c062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_graph_embedding_features(rank_df: pd.DataFrame, item_emb_df: pd.DataFrame, user_prof_df: pd.DataFrame, k: int = 16):\n",
    "    df = rank_df.copy()\n",
    "    emb_cols = [f\"gsvd_{d}\" for d in range(k)]\n",
    "    user_cols = [f\"user_gsvd_{d}\" for d in range(k)]\n",
    "\n",
    "    df = df.merge(item_emb_df, on=\"article_id\", how=\"left\")\n",
    "    df = df.merge(user_prof_df, on=\"customer_id\", how=\"left\")\n",
    "\n",
    "    # fill missing embeddings with 0 (items/users not in vocab)\n",
    "    for c in emb_cols:\n",
    "        df[c] = df[c].fillna(0.0).astype(np.float32)\n",
    "    for c in user_cols:\n",
    "        df[c] = df[c].fillna(0.0).astype(np.float32)\n",
    "\n",
    "    # cosine(user_profile, item_emb) since both are L2-normalized\n",
    "    # (if zero vector, cosine=0)\n",
    "    U = df[user_cols].values\n",
    "    V = df[emb_cols].values\n",
    "    df[\"ui_gsvd_cosine\"] = (U * V).sum(axis=1).astype(np.float32)\n",
    "\n",
    "    # drop user embedding columns to keep feature space smaller (optional)\n",
    "    df.drop(columns=user_cols, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "rank_df_train_g = add_graph_embedding_features(rank_df_train_f, ITEM_EMB_16_DF, USER_PROF_16, k=16)\n",
    "rank_df_val_g   = add_graph_embedding_features(rank_df_val_f,   ITEM_EMB_16_DF, USER_PROF_16, k=16)\n",
    "\n",
    "save_pickle(rank_df_train_g, os.path.join(TRAIN_DIR, \"rank_df_train_features_plus_graph16.pkl\"), overwrite=True)\n",
    "save_pickle(rank_df_val_g,   os.path.join(TRAIN_DIR, \"rank_df_valid_features_plus_graph16.pkl\"), overwrite=True)\n",
    "\n",
    "print(\"Added graph features. Columns now:\", len(rank_df_train_g.columns))\n",
    "rank_df_train_g[[\"gsvd_0\",\"gsvd_1\",\"ui_gsvd_cosine\"]].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e4520b",
   "metadata": {},
   "outputs": [],
   "source": [
    "F_ALL = [\n",
    "    'tx_cnt', 'unique_items', 'recency_days',\n",
    "    'product_type_no', 'colour_group_code', 'department_no', 'index_group_no', 'garment_group_no', 'product_group_id',\n",
    "    'item_popularity', 'unique_buyers', 'item_recency_days',\n",
    "    'ui_cnt', 'ui_recency_days'\n",
    "]\n",
    "F_USER = ['tx_cnt', 'unique_items', 'recency_days']\n",
    "F_ITEM_ATTR = ['product_type_no', 'product_group_id', 'colour_group_code', 'department_no', 'index_group_no', 'garment_group_no']\n",
    "F_ITEM_POP = ['item_popularity', 'unique_buyers', 'item_recency_days']\n",
    "F_UI = ['ui_cnt', 'ui_recency_days']\n",
    "\n",
    "FEATURE_SETS = {\n",
    "    \"all\": F_ALL,\n",
    "    \"user_only\": F_USER,\n",
    "    \"item_attr_only\": F_ITEM_ATTR,\n",
    "    \"item_pop_only\": F_ITEM_POP,\n",
    "    \"ui_only\": F_UI,\n",
    "    \"user+ui\": F_USER + F_UI,\n",
    "    \"item_attr+pop\": F_ITEM_ATTR + F_ITEM_POP,\n",
    "    \"all_minus_ui\": [c for c in F_ALL if c not in set(F_UI)],\n",
    "    \"all_minus_pop\": [c for c in F_ALL if c not in set(F_ITEM_POP)],\n",
    "}\n",
    "\n",
    "print({k: len(v) for k, v in FEATURE_SETS.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0531e1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "F_GRAPH_16 = [f\"gsvd_{d}\" for d in range(16)] + [\"ui_gsvd_cosine\"]\n",
    "\n",
    "# Extend your feature sets\n",
    "FEATURE_SETS[\"all_plus_graph16\"] = FEATURE_SETS[\"all\"] + F_GRAPH_16\n",
    "FEATURE_SETS[\"item_pop_plus_graph16\"] = FEATURE_SETS[\"item_pop_only\"] + [f\"gsvd_{d}\" for d in range(16)]  # optional variant\n",
    "\n",
    "print(\"all_plus_graph16:\", len(FEATURE_SETS[\"all_plus_graph16\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9721f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_sizes_from_sorted(df, group_key=\"customer_id\"):\n",
    "    return df.groupby(group_key).size().to_list()\n",
    "\n",
    "def make_lgb_data(rank_df, feature_cols):\n",
    "    df = ensure_sorted_by_group(rank_df, group_key=\"customer_id\")\n",
    "    X = df[feature_cols]\n",
    "    y = df[\"label\"].astype(np.int8)\n",
    "    group = group_sizes_from_sorted(df, group_key=\"customer_id\")\n",
    "    return df, X, y, group\n",
    "\n",
    "def append_result_row(row, csv_path=RESULT_CSV):\n",
    "    df = pd.DataFrame([row])\n",
    "    if os.path.exists(csv_path):\n",
    "        df.to_csv(csv_path, mode=\"a\", header=False, index=False)\n",
    "    else:\n",
    "        df.to_csv(csv_path, index=False)\n",
    "    print(f\"[appended] {csv_path}\")\n",
    "    \n",
    "def apk(actual, predicted, k=12):\n",
    "    # actual: set/list of true items\n",
    "    # predicted: list of predicted items\n",
    "    if len(predicted) > k:\n",
    "        predicted = predicted[:k]\n",
    "    score = 0.0\n",
    "    hits = 0.0\n",
    "    for i, p in enumerate(predicted, start=1):\n",
    "        if p in actual:\n",
    "            hits += 1.0\n",
    "            score += hits / i\n",
    "    denom = min(len(actual), k)\n",
    "    return score / denom if denom > 0 else 0.0\n",
    "\n",
    "def mapk_from_scored_df(df, k=12):\n",
    "    # df columns: customer_id, article_id, label, score\n",
    "    # label is 1 for relevant\n",
    "    df = df.sort_values([\"customer_id\", \"score\"], ascending=[True, False])\n",
    "    gt = df[df[\"label\"] == 1].groupby(\"customer_id\")[\"article_id\"].apply(list).to_dict()\n",
    "    pred = df.groupby(\"customer_id\")[\"article_id\"].apply(list).to_dict()\n",
    "\n",
    "    scores = []\n",
    "    for cust, pred_list in pred.items():\n",
    "        actual = set(gt.get(cust, []))\n",
    "        scores.append(apk(actual, pred_list, k=k))\n",
    "    return float(np.mean(scores)) if scores else 0.0\n",
    "\n",
    "def ensure_sorted_by_group(df, group_key=\"customer_id\"):\n",
    "    # LightGBM ranking expects grouped rows contiguous.\n",
    "    return df.sort_values([group_key]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7592d8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(\n",
    "    exp_id: str,\n",
    "    exp_name: str,\n",
    "    recall_name: str,\n",
    "    feature_cols: list,\n",
    "    lgb_params: dict,\n",
    "    rank_df_train: pd.DataFrame,\n",
    "    rank_df_val: pd.DataFrame,\n",
    "    save_model: bool = True,\n",
    "):\n",
    "    start = datetime.now()\n",
    "\n",
    "    # build lgb data\n",
    "    train_sorted, X_train, y_train, group_train = make_lgb_data(rank_df_train, feature_cols)\n",
    "    val_sorted, X_val, y_val, group_val = make_lgb_data(rank_df_val, feature_cols)\n",
    "\n",
    "    # train\n",
    "    ranker = lgb.LGBMRanker(**lgb_params)\n",
    "    ranker.fit(\n",
    "        X_train, y_train,\n",
    "        group=group_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        eval_group=[group_val],\n",
    "        eval_at=lgb_params.get(\"eval_at\", [12]),\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=50, verbose=True),\n",
    "            lgb.log_evaluation(period=50)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    best_iter = getattr(ranker, \"best_iteration_\", None)\n",
    "    train_time_sec = (datetime.now() - start).total_seconds()\n",
    "\n",
    "    # eval\n",
    "    val_scores = ranker.predict(X_val, num_iteration=best_iter)\n",
    "    tmp = val_sorted[[\"customer_id\", \"article_id\", \"label\"]].copy()\n",
    "    tmp[\"score\"] = val_scores\n",
    "    manual_map12 = mapk_from_scored_df(tmp, k=12)\n",
    "\n",
    "    # recall stats on valid customers (for logging)\n",
    "    valid_customers = set(valid_gt.keys())\n",
    "    cand_counts = [len(cust_to_candidates.get(c, [])) for c in valid_customers]\n",
    "    val_avg_cand = float(np.mean(cand_counts)) if cand_counts else 0.0\n",
    "    val_med_cand = float(np.median(cand_counts)) if cand_counts else 0.0\n",
    "    val_min_cand = int(np.min(cand_counts)) if cand_counts else 0\n",
    "    val_max_cand = int(np.max(cand_counts)) if cand_counts else 0\n",
    "    val_pos_rate = float(rank_df_val[\"label\"].mean())\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    # save config\n",
    "    cfg = {\n",
    "        \"exp_id\": exp_id,\n",
    "        \"exp_name\": exp_name,\n",
    "        \"recall_name\": recall_name,\n",
    "        \"timestamp\": timestamp,\n",
    "        \"feature_cols\": feature_cols,\n",
    "        \"lgb_params\": lgb_params,\n",
    "        \"num_train_rows\": int(X_train.shape[0]),\n",
    "        \"num_val_rows\": int(X_val.shape[0]),\n",
    "        \"num_features\": int(X_train.shape[1]),\n",
    "        \"valid_start\": str(VALID_START.date()),\n",
    "        \"valid_end\": str((VALID_END - pd.Timedelta(days=1)).date()),\n",
    "        \"train_label_start\": str(TRAIN_LABEL_START.date()),\n",
    "    }\n",
    "    cfg_path = os.path.join(CFG_DIR, f\"{exp_id}_{exp_name}.json\")\n",
    "    save_json(cfg, cfg_path, overwrite=True)\n",
    "\n",
    "    # save model\n",
    "    model_path = None\n",
    "    if save_model:\n",
    "        model_path = os.path.join(MODEL_DIR, f\"{exp_id}_{exp_name}.pkl\")\n",
    "        save_pickle(ranker, model_path, overwrite=True)\n",
    "\n",
    "    row = {\n",
    "        \"exp_id\": exp_id,\n",
    "        \"exp_name\": exp_name,\n",
    "        \"recall_name\": recall_name,\n",
    "        \"timestamp\": timestamp,\n",
    "        \"best_iteration\": int(best_iter) if best_iter is not None else None,\n",
    "        \"train_time_sec\": round(train_time_sec, 2),\n",
    "        \"val_map12_manual\": manual_map12,\n",
    "        \"num_train_rows\": int(X_train.shape[0]),\n",
    "        \"num_val_rows\": int(X_val.shape[0]),\n",
    "        \"num_features\": int(X_train.shape[1]),\n",
    "        \"model_path\": model_path,\n",
    "        \"config_path\": cfg_path,\n",
    "        \"val_avg_candidates\": val_avg_cand,\n",
    "        \"val_med_candidates\": val_med_cand,\n",
    "        \"val_min_candidates\": val_min_cand,\n",
    "        \"val_max_candidates\": val_max_cand,\n",
    "        \"val_pos_rate\": val_pos_rate,\n",
    "    }\n",
    "    append_result_row(row)\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde2ce79",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "BASE_PARAMS = dict(\n",
    "    objective=\"lambdarank\",\n",
    "    metric=\"map\",\n",
    "    eval_at=[12],\n",
    "    learning_rate=0.05,\n",
    "    n_estimators=500,\n",
    "    num_leaves=63,\n",
    "    min_data_in_leaf=50,\n",
    "    feature_fraction=0.8,\n",
    "    bagging_fraction=0.8,\n",
    "    bagging_freq=1,\n",
    "    reg_lambda=1.0,\n",
    "    random_state=SEED,\n",
    "    bagging_seed=SEED,\n",
    "    feature_fraction_seed=SEED,\n",
    "    data_random_seed=SEED,\n",
    "    n_jobs=-1,\n",
    "    # GPU (optional)\n",
    "    # device_type=\"gpu\",\n",
    "    # gpu_platform_id=0,\n",
    "    # gpu_device_id=0,\n",
    ")\n",
    "\n",
    "EXPS_EXTRA = [\n",
    "    (\"E13\", \"all_plus_graph16\", \"all_plus_graph16\", {}),\n",
    "    (\"E14\", \"stronger_reg_plus_graph16\", \"all_plus_graph16\", {\"min_data_in_leaf\":200, \"reg_lambda\":5.0, \"reg_alpha\":1.0}),\n",
    "]\n",
    "RECALL_NAME = \"final_merged_recall_v2\"\n",
    "\n",
    "rows_extra = []\n",
    "for exp_id, exp_name, feat_set_name, override in EXPS_EXTRA:\n",
    "    feats = FEATURE_SETS[feat_set_name]\n",
    "    params = {**BASE_PARAMS, **override}\n",
    "\n",
    "    row = run_experiment(\n",
    "        exp_id=exp_id,\n",
    "        exp_name=exp_name,\n",
    "        recall_name=RECALL_NAME,\n",
    "        feature_cols=feats,\n",
    "        lgb_params=params,\n",
    "        rank_df_train=rank_df_train_g,   # <-- graph-added\n",
    "        rank_df_val=rank_df_val_g,       # <-- graph-added\n",
    "        save_model=True\n",
    "    )\n",
    "    rows_extra.append(row)\n",
    "    print(exp_id, exp_name, \"MAP@12 =\", row[\"val_map12_manual\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hm_rec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
