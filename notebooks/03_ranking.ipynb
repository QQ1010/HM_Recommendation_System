{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7df38327",
   "metadata": {},
   "source": [
    "# H&M Recommendation System — 03 Ranking\n",
    "\n",
    "In this notebook, based on (customer, candidate_item) pair generated by `02_recall.ipynb`to create feature and label and then train ranking model(LightGBM Ranker)\n",
    "- Train window: <= 2020-09-15\n",
    "- Valid window: 2020-09-16 ~ 2020-09-22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "513789f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guest/r12944014/miniconda3/envs/hm_rec/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "DATA_DIR = \"../data\"\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, \"train\")\n",
    "RECALL_DIR = os.path.join(DATA_DIR, \"recall\")\n",
    "HM_DATA_DIR = \"../hm_data\"\n",
    "\n",
    "OUT_DIR = \"../experiments\"\n",
    "MODEL_DIR = \"../models\"\n",
    "CFG_DIR = \"../experiments/configs\"\n",
    "RESULT_CSV = \"../experiments/results.csv\"\n",
    "\n",
    "os.makedirs(TRAIN_DIR, exist_ok=True)\n",
    "os.makedirs(HM_DATA_DIR, exist_ok=True)\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(CFG_DIR, exist_ok=True)\n",
    "\n",
    "VALID_START = pd.to_datetime(\"2020-09-16\")\n",
    "VALID_END = pd.to_datetime(\"2020-09-23\")  # exclusive\n",
    "\n",
    "# Internal train label window (last 7 days before VALID_START)\n",
    "TRAIN_LABEL_START = VALID_START - pd.Timedelta(days=7)\n",
    "\n",
    "# Recall sizes\n",
    "N_HISTORY = 30\n",
    "N_POP = 20\n",
    "N_CATEGORY = 20\n",
    "N_COPURCHASE = 30\n",
    "MAX_CANDIDATES = 100   # final merge cap per user\n",
    "\n",
    "# Category mapping source column\n",
    "CATEGORY_COL = \"product_type_no\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efc5fa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pickle(obj, path, overwrite=False):\n",
    "    if (not overwrite) and os.path.exists(path):\n",
    "        print(f\"[skip] exists: {path}\")\n",
    "        return\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(obj, f)\n",
    "    print(f\"[saved] {path}\")\n",
    "\n",
    "def load_pickle(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def save_json(obj, path, overwrite=False):\n",
    "    if (not overwrite) and os.path.exists(path):\n",
    "        print(f\"[skip] exists: {path}\")\n",
    "        return\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"[saved] {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccba8eee",
   "metadata": {},
   "source": [
    "### Step 1: Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "070d448e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train window: 2018-09-20 00:00:00 -> 2020-09-15 00:00:00 (31548013, 5)\n",
      "Valid window: 2020-09-16 00:00:00 -> 2020-09-22 00:00:00 (240311, 5)\n"
     ]
    }
   ],
   "source": [
    "train_df = load_pickle(os.path.join(TRAIN_DIR, \"train_df.pkl\"))\n",
    "valid_df = load_pickle(os.path.join(TRAIN_DIR, \"valid_df.pkl\"))\n",
    "\n",
    "# Make sure types are correct\n",
    "train_df[\"t_dat\"] = pd.to_datetime(train_df[\"t_dat\"])\n",
    "valid_df[\"t_dat\"] = pd.to_datetime(valid_df[\"t_dat\"])\n",
    "\n",
    "print(\"Train window:\", train_df[\"t_dat\"].min(), \"->\", train_df[\"t_dat\"].max(), train_df.shape)\n",
    "print(\"Valid window:\", valid_df[\"t_dat\"].min(), \"->\", valid_df[\"t_dat\"].max(), valid_df.shape)\n",
    "\n",
    "# ground truth dicts: customer -> set(article_id)\n",
    "train_hist = train_df[train_df[\"t_dat\"] < TRAIN_LABEL_START].copy()\n",
    "train_label_period = train_df[(train_df[\"t_dat\"] >= TRAIN_LABEL_START) & (train_df[\"t_dat\"] < VALID_START)].copy()\n",
    "train_gt = train_label_period.groupby(\"customer_id\")[\"article_id\"].apply(lambda s: set(s.astype(int))).to_dict()\n",
    "valid_period = valid_df[(valid_df[\"t_dat\"] >= VALID_START) & (valid_df[\"t_dat\"] < VALID_END)].copy()\n",
    "valid_gt = valid_period.groupby(\"customer_id\")[\"article_id\"].apply(lambda s: set(s.astype(int))).to_dict()\n",
    "\n",
    "cust_to_candidates = load_pickle(os.path.join(RECALL_DIR, \"recall_final_merged.pkl\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2367b890",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_path = os.path.join(HM_DATA_DIR, \"articles.csv\")\n",
    "articles = pd.read_csv(articles_path)\n",
    "if \"product_group_id\" not in articles.columns:\n",
    "    articles[\"product_group_id\"] = pd.factorize(articles[\"product_group_name\"])[0].astype(np.int32)\n",
    "articles_use = articles[[\n",
    "    \"article_id\",\n",
    "    \"product_type_no\",\n",
    "    \"product_group_id\",\n",
    "    \"colour_group_code\",\n",
    "    \"department_no\",\n",
    "    \"index_group_no\",\n",
    "    \"garment_group_no\",\n",
    "]].copy()\n",
    "articles_use[\"article_id\"] = articles_use[\"article_id\"].astype(np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443670ff",
   "metadata": {},
   "source": [
    "### Step 2: Build ranking datasets (train / valid)\n",
    "labels:\n",
    "- train labels from train_gt (last 7 days inside train)\n",
    "- valid labels from valid_gt (2020-09-16 ~ 2020-09-22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33be0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building rank_train: 100%|██████████| 1362281/1362281 [01:39<00:00, 13726.53it/s]\n",
      "Building rank_valid: 100%|██████████| 1362281/1362281 [01:45<00:00, 12925.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank_df_train: (269726031, 3) pos_rate: 0.0008449685006487194\n",
      "rank_df_val: (269726031, 3) pos_rate: 0.00014426861158239488\n",
      "[saved] ../data/train/rank_df_train.pkl\n",
      "[saved] ../data/train/rank_df_valid.pkl\n"
     ]
    }
   ],
   "source": [
    "# def build_rank_df(cust_to_cand, gt_dict, name=\"rank\"):\n",
    "#     rows = []\n",
    "#     for cust, cands in tqdm(cust_to_cand.items(), desc=f\"Building {name}\"):\n",
    "#         gt = gt_dict.get(cust, set())\n",
    "#         for aid in cands:\n",
    "#             rows.append({\n",
    "#                 \"customer_id\": cust,\n",
    "#                 \"article_id\": int(aid),\n",
    "#                 \"label\": 1 if int(aid) in gt else 0\n",
    "#             })\n",
    "#     df = pd.DataFrame(rows)\n",
    "#     return df\n",
    "\n",
    "# rank_df_train = build_rank_df(cust_to_candidates, train_gt, name=\"rank_train\")\n",
    "# rank_df_val = build_rank_df(cust_to_candidates, valid_gt, name=\"rank_valid\")\n",
    "\n",
    "# print(\"rank_df_train:\", rank_df_train.shape, \"pos_rate:\", rank_df_train[\"label\"].mean())\n",
    "# print(\"rank_df_val:\", rank_df_val.shape, \"pos_rate:\", rank_df_val[\"label\"].mean())\n",
    "\n",
    "# save_pickle(rank_df_train, os.path.join(TRAIN_DIR, \"rank_df_train.pkl\"), overwrite=True)\n",
    "# save_pickle(rank_df_val, os.path.join(TRAIN_DIR, \"rank_df_valid.pkl\"), overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4ed80fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_df_train = load_pickle(os.path.join(TRAIN_DIR, \"rank_df_train.pkl\"))\n",
    "rank_df_val = load_pickle(os.path.join(TRAIN_DIR, \"rank_df_valid.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8ca9ce",
   "metadata": {},
   "source": [
    "### Step 3: Feature engineering (ALL computed from train_hist only)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5609c220",
   "metadata": {},
   "source": [
    "1. User features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26c2d951",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_feat = train_hist.groupby(\"customer_id\").agg(\n",
    "    tx_cnt=(\"article_id\", \"count\"),\n",
    "    unique_items=(\"article_id\", \"nunique\"),\n",
    "    last_date=(\"t_dat\", \"max\"),\n",
    ").reset_index()\n",
    "user_feat[\"recency_days\"] = (VALID_START - user_feat[\"last_date\"]).dt.days.astype(np.int32)\n",
    "user_feat.drop(columns=[\"last_date\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604b19bc",
   "metadata": {},
   "source": [
    "2. Item & popularity features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab9f497c",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_pop = train_hist.groupby(\"article_id\").agg(\n",
    "    item_popularity=(\"customer_id\", \"count\"),\n",
    "    unique_buyers=(\"customer_id\", \"nunique\"),\n",
    "    item_last_date=(\"t_dat\", \"max\"),\n",
    ").reset_index()\n",
    "item_pop[\"item_recency_days\"] = (VALID_START - item_pop[\"item_last_date\"]).dt.days.astype(np.int32)\n",
    "item_pop.drop(columns=[\"item_last_date\"], inplace=True)\n",
    "item_pop[\"article_id\"] = item_pop[\"article_id\"].astype(np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1982306a",
   "metadata": {},
   "source": [
    "3. User-item interaction features (based on train_hist only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1bff9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ui = train_hist.groupby([\"customer_id\", \"article_id\"]).agg(\n",
    "    ui_cnt=(\"t_dat\", \"count\"),\n",
    "    ui_last_date=(\"t_dat\", \"max\"),\n",
    ").reset_index()\n",
    "ui[\"ui_recency_days\"] = (VALID_START - ui[\"ui_last_date\"]).dt.days.astype(np.int32)\n",
    "ui.drop(columns=[\"ui_last_date\"], inplace=True)\n",
    "ui[\"article_id\"] = ui[\"article_id\"].astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1557c7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(rank_df):\n",
    "    df = rank_df.merge(user_feat, on=\"customer_id\", how=\"left\")\n",
    "    df = df.merge(articles_use, on=\"article_id\", how=\"left\")\n",
    "    df = df.merge(item_pop, on=\"article_id\", how=\"left\")\n",
    "    df = df.merge(ui, on=[\"customer_id\", \"article_id\"], how=\"left\")\n",
    "\n",
    "    # Fill missing (cold) values\n",
    "    for c in tqdm(\n",
    "        [\"tx_cnt\",\"unique_items\",\"recency_days\"],\n",
    "        desc=\"Fill user cold features\"\n",
    "    ):\n",
    "        df[c] = df[c].fillna(0).astype(np.int32)\n",
    "\n",
    "    for c in tqdm(\n",
    "        [\"item_popularity\",\"unique_buyers\",\"item_recency_days\"],\n",
    "        desc=\"Fill item cold features\"\n",
    "    ):\n",
    "        df[c] = df[c].fillna(0).astype(np.int32)\n",
    "\n",
    "    for c in tqdm(\n",
    "        [\"ui_cnt\",\"ui_recency_days\"],\n",
    "        desc=\"Fill UI features\"\n",
    "    ):\n",
    "        df[c] = df[c].fillna(0).astype(np.int32)\n",
    "\n",
    "    # Item attrs missing\n",
    "    attr_cols = [\"product_type_no\",\"product_group_id\",\"colour_group_code\",\"department_no\",\"index_group_no\",\"garment_group_no\"]\n",
    "    for c in attr_cols:\n",
    "        df[c] = df[c].fillna(-1).astype(np.int32)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72aee98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] ../data/train/rank_df_train_features.pkl\n",
      "[saved] ../data/train/rank_df_valid_features.pkl\n",
      "rank_df_train_f columns: 17\n"
     ]
    }
   ],
   "source": [
    "# rank_df_train_f = add_features(rank_df_train)\n",
    "# rank_df_val_f = add_features(rank_df_val)\n",
    "\n",
    "# save_pickle(rank_df_train_f, os.path.join(TRAIN_DIR, \"rank_df_train_features.pkl\"), overwrite=True)\n",
    "# save_pickle(rank_df_val_f, os.path.join(TRAIN_DIR, \"rank_df_valid_features.pkl\"), overwrite=True)\n",
    "\n",
    "# print(\"rank_df_train_f columns:\", len(rank_df_train_f.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a49c66a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_df_train_f = load_pickle(os.path.join(TRAIN_DIR, \"rank_df_train_features.pkl\"))\n",
    "rank_df_val_f = load_pickle(os.path.join(TRAIN_DIR, \"rank_df_valid_features.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150e8c69",
   "metadata": {},
   "source": [
    "### Step 4: Train LightBGM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd908ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'all': 14, 'user_only': 3, 'item_attr_only': 6, 'item_pop_only': 3, 'ui_only': 2, 'user+ui': 5, 'item_attr+pop': 9, 'all_minus_ui': 12, 'all_minus_pop': 11}\n"
     ]
    }
   ],
   "source": [
    "F_ALL = [\n",
    "    'tx_cnt', 'unique_items', 'recency_days',\n",
    "    'product_type_no', 'colour_group_code', 'department_no', 'index_group_no', 'garment_group_no', 'product_group_id',\n",
    "    'item_popularity', 'unique_buyers', 'item_recency_days',\n",
    "    'ui_cnt', 'ui_recency_days'\n",
    "]\n",
    "F_USER = ['tx_cnt', 'unique_items', 'recency_days']\n",
    "F_ITEM_ATTR = ['product_type_no', 'product_group_id', 'colour_group_code', 'department_no', 'index_group_no', 'garment_group_no']\n",
    "F_ITEM_POP = ['item_popularity', 'unique_buyers', 'item_recency_days']\n",
    "F_UI = ['ui_cnt', 'ui_recency_days']\n",
    "\n",
    "FEATURE_SETS = {\n",
    "    \"all\": F_ALL,\n",
    "    \"user_only\": F_USER,\n",
    "    \"item_attr_only\": F_ITEM_ATTR,\n",
    "    \"item_pop_only\": F_ITEM_POP,\n",
    "    \"ui_only\": F_UI,\n",
    "    \"user+ui\": F_USER + F_UI,\n",
    "    \"item_attr+pop\": F_ITEM_ATTR + F_ITEM_POP,\n",
    "    \"all_minus_ui\": [c for c in F_ALL if c not in set(F_UI)],\n",
    "    \"all_minus_pop\": [c for c in F_ALL if c not in set(F_ITEM_POP)],\n",
    "}\n",
    "\n",
    "print({k: len(v) for k, v in FEATURE_SETS.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea762f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_sizes_from_sorted(df, group_key=\"customer_id\"):\n",
    "    return df.groupby(group_key).size().to_list()\n",
    "\n",
    "def make_lgb_data(rank_df, feature_cols):\n",
    "    df = ensure_sorted_by_group(rank_df, group_key=\"customer_id\")\n",
    "    X = df[feature_cols]\n",
    "    y = df[\"label\"].astype(np.int8)\n",
    "    group = group_sizes_from_sorted(df, group_key=\"customer_id\")\n",
    "    return df, X, y, group\n",
    "\n",
    "def append_result_row(row, csv_path=RESULT_CSV):\n",
    "    df = pd.DataFrame([row])\n",
    "    if os.path.exists(csv_path):\n",
    "        df.to_csv(csv_path, mode=\"a\", header=False, index=False)\n",
    "    else:\n",
    "        df.to_csv(csv_path, index=False)\n",
    "    print(f\"[appended] {csv_path}\")\n",
    "    \n",
    "def apk(actual, predicted, k=12):\n",
    "    # actual: set/list of true items\n",
    "    # predicted: list of predicted items\n",
    "    if len(predicted) > k:\n",
    "        predicted = predicted[:k]\n",
    "    score = 0.0\n",
    "    hits = 0.0\n",
    "    for i, p in enumerate(predicted, start=1):\n",
    "        if p in actual:\n",
    "            hits += 1.0\n",
    "            score += hits / i\n",
    "    denom = min(len(actual), k)\n",
    "    return score / denom if denom > 0 else 0.0\n",
    "\n",
    "def mapk_from_scored_df(df, k=12):\n",
    "    # df columns: customer_id, article_id, label, score\n",
    "    # label is 1 for relevant\n",
    "    df = df.sort_values([\"customer_id\", \"score\"], ascending=[True, False])\n",
    "    gt = df[df[\"label\"] == 1].groupby(\"customer_id\")[\"article_id\"].apply(list).to_dict()\n",
    "    pred = df.groupby(\"customer_id\")[\"article_id\"].apply(list).to_dict()\n",
    "\n",
    "    scores = []\n",
    "    for cust, pred_list in pred.items():\n",
    "        actual = set(gt.get(cust, []))\n",
    "        scores.append(apk(actual, pred_list, k=k))\n",
    "    return float(np.mean(scores)) if scores else 0.0\n",
    "\n",
    "def ensure_sorted_by_group(df, group_key=\"customer_id\"):\n",
    "    # LightGBM ranking expects grouped rows contiguous.\n",
    "    return df.sort_values([group_key]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e1f16f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(\n",
    "    exp_id: str,\n",
    "    exp_name: str,\n",
    "    recall_name: str,\n",
    "    feature_cols: list,\n",
    "    lgb_params: dict,\n",
    "    rank_df_train: pd.DataFrame,\n",
    "    rank_df_val: pd.DataFrame,\n",
    "    save_model: bool = True,\n",
    "):\n",
    "    start = datetime.now()\n",
    "\n",
    "    # build lgb data\n",
    "    train_sorted, X_train, y_train, group_train = make_lgb_data(rank_df_train, feature_cols)\n",
    "    val_sorted, X_val, y_val, group_val = make_lgb_data(rank_df_val, feature_cols)\n",
    "\n",
    "    # train\n",
    "    ranker = lgb.LGBMRanker(**lgb_params)\n",
    "    ranker.fit(\n",
    "        X_train, y_train,\n",
    "        group=group_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        eval_group=[group_val],\n",
    "        eval_at=lgb_params.get(\"eval_at\", [12]),\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=50, verbose=True),\n",
    "            lgb.log_evaluation(period=50)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    best_iter = getattr(ranker, \"best_iteration_\", None)\n",
    "    train_time_sec = (datetime.now() - start).total_seconds()\n",
    "\n",
    "    # eval\n",
    "    val_scores = ranker.predict(X_val, num_iteration=best_iter)\n",
    "    tmp = val_sorted[[\"customer_id\", \"article_id\", \"label\"]].copy()\n",
    "    tmp[\"score\"] = val_scores\n",
    "    manual_map12 = mapk_from_scored_df(tmp, k=12)\n",
    "\n",
    "    # recall stats on valid customers (for logging)\n",
    "    valid_customers = set(valid_gt.keys())\n",
    "    cand_counts = [len(cust_to_candidates.get(c, [])) for c in valid_customers]\n",
    "    val_avg_cand = float(np.mean(cand_counts)) if cand_counts else 0.0\n",
    "    val_med_cand = float(np.median(cand_counts)) if cand_counts else 0.0\n",
    "    val_min_cand = int(np.min(cand_counts)) if cand_counts else 0\n",
    "    val_max_cand = int(np.max(cand_counts)) if cand_counts else 0\n",
    "    val_pos_rate = float(rank_df_val[\"label\"].mean())\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    # save config\n",
    "    cfg = {\n",
    "        \"exp_id\": exp_id,\n",
    "        \"exp_name\": exp_name,\n",
    "        \"recall_name\": recall_name,\n",
    "        \"timestamp\": timestamp,\n",
    "        \"feature_cols\": feature_cols,\n",
    "        \"lgb_params\": lgb_params,\n",
    "        \"num_train_rows\": int(X_train.shape[0]),\n",
    "        \"num_val_rows\": int(X_val.shape[0]),\n",
    "        \"num_features\": int(X_train.shape[1]),\n",
    "        \"valid_start\": str(VALID_START.date()),\n",
    "        \"valid_end\": str((VALID_END - pd.Timedelta(days=1)).date()),\n",
    "        \"train_label_start\": str(TRAIN_LABEL_START.date()),\n",
    "    }\n",
    "    cfg_path = os.path.join(CFG_DIR, f\"{exp_id}_{exp_name}.json\")\n",
    "    save_json(cfg, cfg_path, overwrite=True)\n",
    "\n",
    "    # save model\n",
    "    model_path = None\n",
    "    if save_model:\n",
    "        model_path = os.path.join(MODEL_DIR, f\"{exp_id}_{exp_name}.pkl\")\n",
    "        save_pickle(ranker, model_path, overwrite=True)\n",
    "\n",
    "    row = {\n",
    "        \"exp_id\": exp_id,\n",
    "        \"exp_name\": exp_name,\n",
    "        \"recall_name\": recall_name,\n",
    "        \"timestamp\": timestamp,\n",
    "        \"best_iteration\": int(best_iter) if best_iter is not None else None,\n",
    "        \"train_time_sec\": round(train_time_sec, 2),\n",
    "        \"val_map12_manual\": manual_map12,\n",
    "        \"num_train_rows\": int(X_train.shape[0]),\n",
    "        \"num_val_rows\": int(X_val.shape[0]),\n",
    "        \"num_features\": int(X_train.shape[1]),\n",
    "        \"model_path\": model_path,\n",
    "        \"config_path\": cfg_path,\n",
    "        \"val_avg_candidates\": val_avg_cand,\n",
    "        \"val_med_candidates\": val_med_cand,\n",
    "        \"val_min_candidates\": val_min_cand,\n",
    "        \"val_max_candidates\": val_max_cand,\n",
    "        \"val_pos_rate\": val_pos_rate,\n",
    "    }\n",
    "    append_result_row(row)\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eaf1d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running LGBM Rank Experiments:   0%|          | 0/1 [00:00<?, ?it/s]/home/guest/r12944014/miniconda3/envs/hm_rec/lib/python3.10/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Info] Total groups: 1362281, total data: 269726031\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 3.478570 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2155\n",
      "[LightGBM] [Info] Number of data points in the train set: 269726031, number of used features: 14\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Info] Total groups: 1362281, total data: 269726031\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\tvalid_0's map@12: 0.981658\n",
      "[100]\tvalid_0's map@12: 0.981718\n",
      "[150]\tvalid_0's map@12: 0.9818\n",
      "[200]\tvalid_0's map@12: 0.981846\n",
      "[250]\tvalid_0's map@12: 0.981866\n",
      "[300]\tvalid_0's map@12: 0.981884\n",
      "[350]\tvalid_0's map@12: 0.981904\n",
      "[400]\tvalid_0's map@12: 0.981913\n",
      "[450]\tvalid_0's map@12: 0.981927\n",
      "[500]\tvalid_0's map@12: 0.981935\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\tvalid_0's map@12: 0.981935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guest/r12944014/miniconda3/envs/hm_rec/lib/python3.10/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[saved] ../experiments/configs/E11-v2_lr_0.03_more_trees.json\n",
      "[saved] ../models/E11-v2_lr_0.03_more_trees.pkl\n",
      "[appended] ../experiments/results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running LGBM Rank Experiments: 100%|██████████| 1/1 [51:10<00:00, 3070.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E11-v2 lr_0.03_more_trees all MAP@12 = 0.0017342082720044477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exp_id</th>\n",
       "      <th>exp_name</th>\n",
       "      <th>num_features</th>\n",
       "      <th>best_iteration</th>\n",
       "      <th>train_time_sec</th>\n",
       "      <th>val_map12_manual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E11-v2</td>\n",
       "      <td>lr_0.03_more_trees</td>\n",
       "      <td>14</td>\n",
       "      <td>500</td>\n",
       "      <td>2532.95</td>\n",
       "      <td>0.001734</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   exp_id            exp_name  num_features  best_iteration  train_time_sec  \\\n",
       "0  E11-v2  lr_0.03_more_trees            14             500         2532.95   \n",
       "\n",
       "   val_map12_manual  \n",
       "0          0.001734  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SEED = 42\n",
    "\n",
    "BASE_PARAMS = dict(\n",
    "    objective=\"lambdarank\",\n",
    "    metric=\"map\",\n",
    "    eval_at=[12],\n",
    "    learning_rate=0.05,\n",
    "    n_estimators=500,\n",
    "    num_leaves=63,\n",
    "    min_data_in_leaf=50,\n",
    "    feature_fraction=0.8,\n",
    "    bagging_fraction=0.8,\n",
    "    bagging_freq=1,\n",
    "    reg_lambda=1.0,\n",
    "    random_state=SEED,\n",
    "    bagging_seed=SEED,\n",
    "    feature_fraction_seed=SEED,\n",
    "    data_random_seed=SEED,\n",
    "    n_jobs=-1,\n",
    "    # GPU (optional)\n",
    "    # device_type=\"gpu\",\n",
    "    # gpu_platform_id=0,\n",
    "    # gpu_device_id=0,\n",
    ")\n",
    "\n",
    "EXPS = [\n",
    "    (\"E01\", \"baseline_all\",          \"all\",            {}),\n",
    "    (\"E02\", \"user_only\",             \"user_only\",       {}),\n",
    "    (\"E03\", \"item_attr_only\",        \"item_attr_only\",  {}),\n",
    "    (\"E04\", \"item_pop_only\",         \"item_pop_only\",   {}),\n",
    "    (\"E05\", \"ui_only\",               \"ui_only\",         {}),\n",
    "    (\"E06\", \"user_plus_ui\",          \"user+ui\",         {}),\n",
    "    (\"E07\", \"item_attr_plus_pop\",    \"item_attr+pop\",   {}),\n",
    "    (\"E08\", \"all_minus_ui\",          \"all_minus_ui\",    {}),\n",
    "    (\"E09\", \"all_minus_pop\",         \"all_minus_pop\",   {}),\n",
    "\n",
    "    (\"E10\", \"stronger_reg\",          \"all\", {\"min_data_in_leaf\":200, \"reg_lambda\":5.0, \"reg_alpha\":1.0}),\n",
    "    (\"E11\", \"lr_0.03_more_trees\",    \"all\", {\"learning_rate\":0.03, \"n_estimators\":4000}),\n",
    "    (\"E11-v2\", \"lr_0.03_more_trees\",    \"all\", {\"learning_rate\":0.03}),\n",
    "    (\"E12\", \"extra_trees\",           \"all\", {\"extra_trees\":True}),\n",
    "]\n",
    "\n",
    "# IMPORTANT:\n",
    "# - rank_df_train_f / rank_df_val_f must already contain ALL engineered columns\n",
    "# - FEATURE_SETS maps feat_set_name -> list of feature column names\n",
    "# - run_experiment() is the time-split version (uses rank_df_train/val DataFrames)\n",
    "\n",
    "RECALL_NAME = \"final_merged_recall_v2\"\n",
    "\n",
    "rows = []\n",
    "for exp_id, exp_name, feat_set_name, override in tqdm(\n",
    "    EXPS,\n",
    "    desc=\"Running LGBM Rank Experiments\",\n",
    "    total=len(EXPS)\n",
    "):\n",
    "    feats = FEATURE_SETS[feat_set_name]\n",
    "    params = {**BASE_PARAMS, **override}\n",
    "\n",
    "    row = run_experiment(\n",
    "        exp_id=exp_id,\n",
    "        exp_name=exp_name,\n",
    "        recall_name=RECALL_NAME,\n",
    "        feature_cols=feats,\n",
    "        lgb_params=params,\n",
    "        rank_df_train=rank_df_train_f,\n",
    "        rank_df_val=rank_df_val_f,\n",
    "        save_model=True\n",
    "    )\n",
    "    rows.append(row)\n",
    "    print(exp_id, exp_name, feat_set_name, \"MAP@12 =\", row[\"val_map12_manual\"])\n",
    "\n",
    "# Optional: show as a dataframe summary\n",
    "res_df = pd.DataFrame(rows).sort_values(\"val_map12_manual\", ascending=False)\n",
    "display(res_df[[\"exp_id\",\"exp_name\",\"num_features\",\"best_iteration\",\"train_time_sec\",\"val_map12_manual\"]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hm_rec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
